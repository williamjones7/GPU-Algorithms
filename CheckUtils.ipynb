{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyNetwork import utils\n",
    "import pyopencl as cl\n",
    "import pyopencl.array as cl_array\n",
    "import numpy as np\n",
    "\n",
    "n = 10\n",
    "m = 100\n",
    "d = 20\n",
    "\n",
    "platform = cl.get_platforms()\n",
    "devices = platform[0].get_devices()\n",
    "context = cl.Context(devices)\n",
    "queue = cl.CommandQueue(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random data\n",
    "X = np.random.rand(20, 10).astype(np.float32)\n",
    "Y = np.random.rand(10, 100).astype(np.float32)\n",
    "\n",
    "# Send data to the gpu\n",
    "X_gpu = cl_array.to_device(queue, X)\n",
    "Y_gpu = cl_array.to_device(queue, Y)\n",
    "\n",
    "gpu_maths = utils.ArrayFunctions(context, queue)\n",
    "gpu_matmul = utils.NaiveMatMul(context, queue)\n",
    "\n",
    "# Test the result of matrix multiplication\n",
    "matmul_result = gpu_matmul.naiveMatmul(X_gpu, Y_gpu)\n",
    "np.testing.assert_almost_equal(matmul_result.get(), X @ Y, decimal=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.random.rand(10).astype(np.float32)\n",
    "\n",
    "# Send data to the gpu\n",
    "b_gpu = cl_array.to_device(queue, b)\n",
    "\n",
    "# Test the result of vector addition\n",
    "pred = gpu_maths.addVector(X_gpu, b_gpu)\n",
    "np.testing.assert_almost_equal(pred.get(), X + b, decimal=5)\n",
    "# Test the result of vector subtraction\n",
    "pred = gpu_maths.addVector(X_gpu, -b_gpu)\n",
    "np.testing.assert_almost_equal(pred.get(), X - b, decimal=5)\n",
    "# Test the result of vector multiplication\n",
    "pred = gpu_maths.mulVector(X_gpu, b_gpu)\n",
    "np.testing.assert_almost_equal(pred.get(), X * b, decimal=5)\n",
    "# Test the result of vector division\n",
    "pred = gpu_maths.divVector(X_gpu, b_gpu)\n",
    "np.testing.assert_almost_equal(pred.get(), X / b, decimal=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random data\n",
    "prev_z = np.random.rand(d, m).astype(np.float32)\n",
    "delta = np.random.rand(d, n).astype(np.float32)\n",
    "\n",
    "# Send data to the gpu\n",
    "prev_z_gpu = cl_array.to_device(queue, prev_z)\n",
    "delta_gpu = cl_array.to_device(queue, delta)\n",
    "delta_gpu_T = cl_array.transpose(delta_gpu)\n",
    "\n",
    "# Test the result of gradients\n",
    "weight_result = gpu_matmul.naiveMatmul(delta_gpu_T, prev_z_gpu)\n",
    "np.testing.assert_almost_equal(weight_result.get(), np.ascontiguousarray(delta.T) @ prev_z, decimal=5)\n",
    "bias_result = gpu_maths.rowSumUp(delta_gpu)\n",
    "np.testing.assert_almost_equal(bias_result.get(), np.sum(delta, axis=0), decimal=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random data\n",
    "W = np.random.rand(n, m).astype(np.float32)\n",
    "x = np.random.rand(d, m).astype(np.float32)\n",
    "b = np.random.rand(n).astype(np.float32)\n",
    "input_width, _ = x.shape \n",
    "\n",
    "# Send data to the gpu\n",
    "W_gpu = cl_array.to_device(queue, W)\n",
    "W_gpu_T = cl_array.transpose(W_gpu)\n",
    "x_gpu = cl_array.to_device(queue, x)\n",
    "b_matrix = np.tile(b, (input_width, 1))\n",
    "b_gpu = cl_array.to_device(queue, b_matrix)\n",
    "\n",
    "pred = gpu_matmul.naiveMatmul(x_gpu, W_gpu_T) + b_gpu\n",
    "np.testing.assert_almost_equal(pred.get(), x @ W.T + b, decimal=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random data\n",
    "x = -1 * np.random.rand(100, 10).astype(np.float32)\n",
    "x_gpu = cl_array.to_device(queue, x)\n",
    "\n",
    "# Test the result of the function sign()\n",
    "(gpu_maths.clarray_sign(x_gpu).get() == np.sign(x)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the effect of contiguous\n",
    "delta = np.random.rand(d, m).astype(np.float32)\n",
    "delta_gpu = cl_array.to_device(queue, delta)\n",
    "delta_gpu_F = cl_array.to_device(queue, np.asfortranarray(delta))\n",
    "delta_gpu_T = cl_array.transpose(delta_gpu)\n",
    "weight_grad = gpu_matmul.naiveMatmul(delta_gpu_T, prev_z_gpu)\n",
    "np.testing.assert_almost_equal(weight_grad.get(), np.ascontiguousarray(delta.T) @ prev_z, decimal=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_mean = gpu_maths.rowMean(delta_gpu)\n",
    "np.testing.assert_almost_equal(bias_mean.get(), np.mean(delta, axis=0), decimal=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_std = gpu_maths.rowStd(delta_gpu)\n",
    "np.testing.assert_almost_equal(bias_std.get(), np.std(delta, axis=0), decimal=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
